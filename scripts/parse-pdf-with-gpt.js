#!/usr/bin/env node
/**
 * External PDF Parser using GPT API
 * Usage: node scripts/parse-pdf-with-gpt.js <norm-id> [target-system-id]
 * 
 * This script:
 * 1. Fetches PDF file from database
 * 2. Extracts text using pdf-parse
 * 3. Sends to GPT-4o-mini API for parsing
 * 4. Saves structured requirements to database
 */

require('dotenv').config({ path: '.env' });
require('dotenv').config({ path: '.env.local' });
const { createClient } = require('@supabase/supabase-js');
const OpenAI = require('openai');
const fs = require('fs').promises;
const path = require('path');
const { v4: uuidv4 } = require('uuid');

// Import our robust PDF helper
const { extractPdfText } = require('../lib/pdf-helper-combo');

// Initialize clients
const supabase = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL,
    process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY
);

const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY
});

/**
 * Extract ONLY Russian text from bilingual documents
 * Strategy: Bilingual docs usually have structure [Kazakh part] [Russian part]
 * We find where Russian starts and cut everything before it
 */
function extractRussianText(fullText) {
    // Markers that indicate start of Russian section
    const russianMarkers = [
        '–ü–û–ñ–ê–†–ù–ê–Ø –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨',
        '–ü–û–ñ–ê–†–ù–ê–Ø –ê–í–¢–û–ú–ê–¢–ò–ö–ê',
        '–ü–†–û–¢–ò–í–û–ü–û–ñ–ê–†–ù–ê–Ø –ó–ê–©–ò–¢–ê',
        '–û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø',
        '–û–ë–õ–ê–°–¢–¨ –ü–†–ò–ú–ï–ù–ï–ù–ò–Ø',
        '–ù–û–†–ú–ê–¢–ò–í–ù–´–ï –°–°–´–õ–ö–ò',
        '1 –û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è',
        '2 –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ',
        '–í–í–ï–î–ï–ù–ò–ï'
    ];

    let bestSplitIndex = -1;
    let bestMarker = null;

    // Try to find any of the markers
    for (const marker of russianMarkers) {
        const index = fullText.indexOf(marker);
        if (index > 0 && (bestSplitIndex === -1 || index < bestSplitIndex)) {
            bestSplitIndex = index;
            bestMarker = marker;
        }
    }

    if (bestSplitIndex > 0) {
        console.log(`   ‚úÖ Found Russian marker: "${bestMarker}" at position ${bestSplitIndex}`);
        // Take from the marker onwards
        return fullText.substring(bestSplitIndex);
    }

    // Fallback: If no marker found, try to detect by language ratio
    // Split in half and check which half has more Cyrillic
    const midPoint = Math.floor(fullText.length / 2);
    const firstHalf = fullText.substring(0, midPoint);
    const secondHalf = fullText.substring(midPoint);

    const cyrillicPattern = /[–∞-—è–ê-–Ø—ë–Å]/g;
    const latinPattern = /[a-zA-Z]/g;

    const firstCyrillic = (firstHalf.match(cyrillicPattern) || []).length;
    const secondCyrillic = (secondHalf.match(cyrillicPattern) || []).length;

    // Kazakh uses more Latin characters, Russian uses pure Cyrillic
    // So second half (Russian) should have MORE Cyrillic
    if (secondCyrillic > firstCyrillic * 1.2) {
        console.log(`   ‚ÑπÔ∏è  No marker found, using language detection split`);
        console.log(`   First half Cyrillic: ${firstCyrillic}, Second half: ${secondCyrillic}`);
        return secondHalf;
    }

    // If all fails, return as is (maybe it's Russian-only document)
    console.log(`   ‚ö†Ô∏è  Could not determine bilingual structure, keeping full text`);
    return fullText;
}

// ===== MAIN FUNCTION =====
// Parse command line arguments
const normId = process.argv[2];
const targetSystemId = process.argv[3] || null;

if (!normId) {
    console.error('‚ùå Usage: node scripts/parse-pdf-with-gpt.js <norm-id> [target-system-id]');
    process.exit(1);
}

console.log('\nüöÄ External PDF Parser with GPT');
console.log('================================\n');
console.log(`üìã Norm ID: ${normId}`);
if (targetSystemId) {
    console.log(`üéØ Target System: ${targetSystemId}`);
}
console.log('');

async function parsePdfWithGpt() {
    try {
        // Step 1: Fetch norm details
        console.log('üìñ Step 1: Fetching norm details...');
        const { data: norm, error: normError } = await supabase
            .from('norm_sources')
            .select('*')
            .eq('id', normId)
            .single();

        if (normError || !norm) {
            throw new Error('Norm not found');
        }

        console.log(`   ‚úÖ Found: ${norm.code} - ${norm.title}\n`);

        // Step 2: Fetch PDF files
        console.log('üìÅ Step 2: Fetching PDF files...');
        const { data: files, error: filesError } = await supabase
            .from('norm_files')
            .select('*')
            .eq('normSourceId', normId)
            .order('uploadedAt', { ascending: false });

        if (filesError || !files || files.length === 0) {
            throw new Error('No PDF files attached to this norm');
        }

        // Filter out ghost files
        const validFiles = files.filter(f =>
            !f.storageUrl.includes('test/data') &&
            !f.storageUrl.includes('test\\data')
        );

        if (validFiles.length === 0) {
            throw new Error('No valid PDF files found');
        }

        console.log(`   ‚úÖ Found ${validFiles.length} valid file(s)\n`);

        // Step 3: Extract text from PDF
        console.log('üìÑ Step 3: Extracting text from PDF...');
        let text = '';
        let fileUsed = null;

        for (const fileRecord of validFiles) {
            const relativePath = fileRecord.storageUrl;
            const cleanPath = relativePath.startsWith('/') ? relativePath.substring(1) : relativePath;
            let absolutePath = path.join(process.cwd(), 'public', cleanPath);

            // Try to access file
            try {
                await fs.access(absolutePath);
            } catch {
                // Fallback to uploads folder
                const fileName = path.basename(relativePath);
                absolutePath = path.join(process.cwd(), 'public', 'uploads', 'norms', fileName);
                try {
                    await fs.access(absolutePath);
                } catch {
                    console.log(`   ‚ö†Ô∏è  Skipping ${fileRecord.fileName} - file not found`);
                    continue;
                }
            }

            console.log(`   üìñ Reading: ${fileRecord.fileName}`);

            try {
                const dataBuffer = await fs.readFile(absolutePath);
                const stats = await fs.stat(absolutePath);
                console.log(`   üìä Size: ${(stats.size / 1024).toFixed(2)} KB`);

                text = await extractPdfText(dataBuffer);
                console.log(`   ‚úÖ Extracted ${text.length} characters\n`);

                if (text && text.length >= 50) {
                    fileUsed = fileRecord;
                    break;
                }
            } catch (err) {
                console.error(`   ‚ùå Error extracting from ${fileRecord.fileName}:`, err.message);
            }
        }

        if (!text || text.length < 50) {
            throw new Error('Could not extract meaningful text from PDF');
        }

        // Step 3.5: Extract ONLY Russian text from bilingual documents
        console.log('üá∑üá∫ Step 3.5: Extracting Russian text only...');
        const originalLength = text.length;
        text = extractRussianText(text);
        console.log(`   ‚úÇÔ∏è  Removed Kazakh part: ${originalLength} ‚Üí ${text.length} chars (${((1 - text.length / originalLength) * 100).toFixed(1)}% reduction)`);
        console.log(`   üí∞ Token savings: ~${Math.floor((originalLength - text.length) / 4)} tokens\n`);

        // Step 4: Send to GPT API
        console.log('ü§ñ Step 4: Sending to GPT-4o-mini for parsing...');
        console.log(`   üìù Sending ${text.length} characters to AI\n`);

        const systemPrompt = `–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –ø–æ–∂–∞—Ä–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö —Å–∏—Å—Ç–µ–º.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

**–í–ê–ñ–ù–û**: 
- –ï—Å–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –¥–≤—É—Ö —è–∑—ã–∫–∞—Ö (–∫–∞–∑–∞—Ö—Å–∫–∏–π –∏ —Ä—É—Å—Å–∫–∏–π), **–ü–†–ò–û–†–ò–¢–ï–¢ –Ω–∞ –†–£–°–°–ö–ò–ô —Ç–µ–∫—Å—Ç**
- –û–±—ã—á–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: —Å–Ω–∞—á–∞–ª–∞ –∫–∞–∑–∞—Ö—Å–∫–∏–π, –ø–æ—Ç–æ–º —Ä—É—Å—Å–∫–∏–π
- –ò–∑–≤–ª–µ–∫–∞–π —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¢–û–õ–¨–ö–û –∏–∑ —Ä—É—Å—Å–∫–æ–π —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞

–í–µ—Ä–Ω–∏ JSON –º–∞—Å—Å–∏–≤ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
[
  {
    "clause": "–ø—É–Ω–∫—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 5.2.1)",
    "system": "–û–°–ù–û–í–ù–ê–Ø —Å–∏—Å—Ç–µ–º–∞ (–æ–¥–Ω–∞)",
    "requirementTextShort": "–∫—Ä–∞—Ç–∫–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
    "requirementTextFull": "–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è",
    "checkMethod": "–º–µ—Ç–æ–¥ –ø—Ä–æ–≤–µ—Ä–∫–∏ (visual/document/test/measurement/log)",
    "mustCheck": true/false,
    "tags": ["–≤—Å–µ_—Å–≤—è–∑–∞–Ω–Ω—ã–µ_—Å–∏—Å—Ç–µ–º—ã", "–∫–ª—é—á–µ–≤—ã–µ_—Å–ª–æ–≤–∞"]
  }
]

**–ü–û–õ–ù–´–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –°–ù/–°–ü –†–ö:**

**–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∂–∞—Ä–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã:**
- FIRE_GENERAL: –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ–∂–∞—Ä–Ω–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–¥–ª—è –æ–±—â–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫–æ –≤—Å–µ–º —Å–∏—Å—Ç–µ–º–∞–º)
- APS: –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–æ–∂–∞—Ä–Ω–æ–π —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ (–ê–ü–°)
- SOUE: –°–∏—Å—Ç–µ–º–∞ –æ–ø–æ–≤–µ—â–µ–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —ç–≤–∞–∫—É–∞—Ü–∏–µ–π –ª—é–¥–µ–π –ø—Ä–∏ –ø–æ–∂–∞—Ä–µ (–°–û–£–≠)
- AUPT: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–æ–∂–∞—Ä–æ—Ç—É—à–µ–Ω–∏—è (–ê–£–ü–¢) - –≤–∫–ª—é—á–∞–µ—Ç –≤–æ–¥—è–Ω—ã–µ, –ø–µ–Ω–Ω—ã–µ, –≥–∞–∑–æ–≤—ã–µ, –ø–æ—Ä–æ—à–∫–æ–≤—ã–µ, –∞—ç—Ä–æ–∑–æ–ª—å–Ω—ã–µ
- SMOKE_CONTROL: –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ—Ç–∏–≤–æ–¥—ã–º–Ω–æ–π –∑–∞—â–∏—Ç—ã –∑–¥–∞–Ω–∏–π –∏ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π
- FIRE_WATER_INT: –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∂–∞—Ä–Ω—ã–π –≤–æ–¥–æ–ø—Ä–æ–≤–æ–¥ (–í–ü–í)
- FIRE_WATER_EXT: –ù–∞—Ä—É–∂–Ω–æ–µ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∂–∞—Ä–Ω–æ–µ –≤–æ–¥–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ
- FIRE_POWER: –≠–ª–µ–∫—Ç—Ä–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∂–∞—Ä–Ω–æ–π –∑–∞—â–∏—Ç—ã
- FIRE_CABLES: –ö–∞–±–µ–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏ –∏ –æ–≥–Ω–µ—Å—Ç–æ–π–∫–∏–µ —Ç—Ä–∞—Å—Å—ã —Å–∏—Å—Ç–µ–º –ü–ë
- FIRE_BARRIERS: –ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∂–∞—Ä–Ω—ã–µ –ø—Ä–µ–≥—Ä–∞–¥—ã (—Å—Ç–µ–Ω—ã, –ø–µ—Ä–µ–≥–æ—Ä–æ–¥–∫–∏, –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è, –¥–≤–µ—Ä–∏, –≤–æ—Ä–æ—Ç–∞, –ª—é–∫–∏)
- FIRE_PRIMARY: –ü–µ—Ä–≤–∏—á–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –ø–æ–∂–∞—Ä–æ—Ç—É—à–µ–Ω–∏—è (–æ–≥–Ω–µ—Ç—É—à–∏—Ç–µ–ª–∏, —â–∏—Ç—ã, –∏–Ω–≤–µ–Ω—Ç–∞—Ä—å)
- FIRE_CONTROL: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–∞–º–∏ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∂–∞—Ä–Ω–æ–π –∑–∞—â–∏—Ç—ã
- FIRE_MONITORING: –°–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ–¥–∞—á–∏ –∏–∑–≤–µ—â–µ–Ω–∏–π –æ –ø–æ–∂–∞—Ä–µ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∏–∑–∞—Ü–∏–∏

**–î—Ä—É–≥–∏–µ –ò.–°.:**
- CCTV: –í–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
- ACS: –ö–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø–∞
- OS: –û—Ö—Ä–∞–Ω–Ω–∞—è —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏—è
- SCS: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–∞–±–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞

**–ö–ê–ö –û–ü–†–ï–î–ï–õ–Ø–¢–¨ –°–ò–°–¢–ï–ú–£:**
1. system - –≤—ã–±–µ—Ä–∏ –û–î–ù–£ –æ—Å–Ω–æ–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É, –∫ –∫–æ—Ç–æ—Ä–æ–π –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ
2. tags - –¥–æ–±–∞–≤—å –í–°–ï —Å–∏—Å—Ç–µ–º—ã –∫–æ—Ç–æ—Ä—ã—Ö –∫–∞—Å–∞–µ—Ç—Å—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞

**–ü—Ä–∏–º–µ—Ä 1** (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ –∫–∞—Å–∞–µ—Ç—Å—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–∏—Å—Ç–µ–º):
{
  "clause": "5.2.1",
  "system": "APS",
  "requirementTextShort": "–ü–æ–∂–∞—Ä–Ω—ã–µ –∏–∑–≤–µ—â–∞—Ç–µ–ª–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–µ—Å–ø–µ—á–µ–Ω—ã —Ä–µ–∑–µ—Ä–≤–Ω—ã–º –ø–∏—Ç–∞–Ω–∏–µ–º",
  "tags": ["–ê–ü–°", "FIRE_POWER", "—ç–ª–µ–∫—Ç—Ä–æ–ø–∏—Ç–∞–Ω–∏–µ", "—Ä–µ–∑–µ—Ä–≤"]
}

**–ü—Ä–∏–º–µ—Ä 2** (–æ–±—â–µ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ):
{
  "clause": "4.1",
  "system": "FIRE_GENERAL",
  "requirementTextShort": "–°–∏—Å—Ç–µ–º—ã –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∂–∞—Ä–Ω–æ–π –∑–∞—â–∏—Ç—ã –¥–æ–ª–∂–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –ø—Ä–æ–µ–∫—Ç—É",
  "tags": ["–æ–±—â–∏–µ_—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "APS", "SOUE", "FIRE_EXTINGUISH", "–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"]
}

–ú–µ—Ç–æ–¥—ã –ø—Ä–æ–≤–µ—Ä–∫–∏:
- visual: –í–∏–∑—É–∞–ª—å–Ω—ã–π –æ—Å–º–æ—Ç—Ä
- document: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- test: –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
- measurement: –ò–∑–º–µ—Ä–µ–Ω–∏—è
- log: –ê–Ω–∞–ª–∏–∑ –∂—É—Ä–Ω–∞–ª–æ–≤

–ò–∑–≤–ª–µ–∫–∞–π —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è. –ò–≥–Ω–æ—Ä–∏—Ä—É–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è, –ø—Ä–µ–∞–º–±—É–ª—ã, —Ç–∏—Ç—É–ª—å–Ω—ã–µ –ª–∏—Å—Ç—ã.`;

        const userPrompt = `–ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç: "${norm.code} - ${norm.title}"
${targetSystemId ? `\n–¶–µ–ª–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞: ${targetSystemId} (–∏–∑–≤–ª–µ–∫–∞–π —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è —ç—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã)\n` : ''}

–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:
${text.substring(0, 100000)}

–í–µ—Ä–Ω–∏ JSON –º–∞—Å—Å–∏–≤ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π.`;

        const completion = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                { role: 'system', content: systemPrompt },
                { role: 'user', content: userPrompt }
            ],
            response_format: { type: 'json_object' },
            temperature: 0.3
        });

        const responseText = completion.choices[0].message.content;
        console.log('   ‚úÖ Received response from GPT\n');

        // Step 5: Parse JSON response
        console.log('üîç Step 5: Parsing AI response...');

        let parsedData;
        try {
            parsedData = JSON.parse(responseText);
        } catch (err) {
            console.error('   ‚ùå Failed to parse JSON:', err.message);
            console.error('   Response:', responseText.substring(0, 500));
            throw new Error('Invalid JSON response from GPT');
        }

        // Handle different response formats
        let requirements = [];
        if (Array.isArray(parsedData)) {
            requirements = parsedData;
        } else if (parsedData.requirements && Array.isArray(parsedData.requirements)) {
            requirements = parsedData.requirements;
        } else if (parsedData.items && Array.isArray(parsedData.items)) {
            requirements = parsedData.items;
        } else {
            console.error('   ‚ùå Unexpected response format:', Object.keys(parsedData));
            throw new Error('Could not find requirements array in response');
        }

        console.log(`   ‚úÖ Extracted ${requirements.length} requirements\n`);

        if (requirements.length === 0) {
            throw new Error('No requirements extracted');
        }

        // Step 6: Save to database
        console.log('üíæ Step 6: Saving to database...');

        // Step 6a: Find requirement set (use existing)
        console.log('   üîç Finding requirement set...');

        // Try to find existing set for this jurisdiction
        let { data: existingSets } = await supabase
            .from('requirement_sets')
            .select('id, requirementSetId')
            .eq('jurisdiction', norm.jurisdiction || 'KZ')
            .limit(1);

        let requirementSetId;

        if (existingSets && existingSets.length > 0) {
            requirementSetId = existingSets[0].id;
            console.log(`   ‚úÖ Using requirement set: ${existingSets[0].requirementSetId}`);
        } else {
            // Use any available set as fallback
            const { data: fallbackSet } = await supabase
                .from('requirement_sets')
                .select('id, requirementSetId')
                .limit(1)
                .single();

            if (fallbackSet) {
                requirementSetId = fallbackSet.id;
                console.log(`   ‚úÖ Using fallback set: ${fallbackSet.requirementSetId}`);
            } else {
                throw new Error('No requirement sets in database');
            }
        }

        // Step 6b: Check for existing requirements (protection against accidental overwrites)
        console.log('   üîç Checking for existing requirements...');
        const { data: existingReqs, error: checkError } = await supabase
            .from('requirements')
            .select('id, createdBy')
            .eq('normSourceId', normId);

        if (existingReqs && existingReqs.length > 0) {
            const manualCount = existingReqs.filter(r => r.createdBy === 'manual').length;
            console.log(`   ‚ö†Ô∏è  WARNING: Found ${existingReqs.length} existing requirements`);
            if (manualCount > 0) {
                console.log(`   ‚ö†Ô∏è  WARNING: ${manualCount} were added manually!`);
            }
            console.log(`   üóëÔ∏è  Deleting old requirements...`);
        }

        // Delete old requirements for this norm
        const { error: deleteError } = await supabase
            .from('requirements')
            .delete()
            .eq('normSourceId', normId);

        if (deleteError) {
            console.warn('   ‚ö†Ô∏è  Warning: Could not delete old requirements:', deleteError.message);
        }

        // Step 6c: Prepare requirements for insertion
        const now = new Date().toISOString();

        // Valid system IDs (must match database)
        const validSystems = ['APS', 'SOUE', 'CCTV', 'ACS', 'OS', 'SCS'];

        // Helper to normalize system ID
        const normalizeSystemId = (system) => {
            if (!system) return 'APS';
            const upper = system.toUpperCase();
            return validSystems.includes(upper) ? upper : 'APS'; // Default to APS if invalid
        };

        const requirementsToInsert = requirements.map((req, index) => ({
            id: uuidv4(), // Generate UUID for primary key
            requirementId: `REQ-${norm.code.replace(/[^A-Z0-9]/gi, '-')}-${String(index + 1).padStart(4, '0')}`,
            requirementSetId: requirementSetId,
            systemId: normalizeSystemId(req.system || targetSystemId),
            normSourceId: normId,
            clause: req.clause || `Section-${index + 1}`,
            requirementTextShort: req.requirementTextShort || req.requirementTextFull?.substring(0, 200) || 'No description',
            requirementTextFull: req.requirementTextFull || req.requirementTextShort || '',
            checkMethod: req.checkMethod || 'visual',
            evidenceTypeExpected: req.evidenceTypeExpected || ['photo'],
            mustCheck: req.mustCheck || false,
            tags: req.tags || [],
            createdBy: 'external-parser',
            createdAt: now,
            updatedAt: now
        }));

        // Insert requirements
        const { data: inserted, error: insertError } = await supabase
            .from('requirements')
            .insert(requirementsToInsert)
            .select();

        if (insertError) {
            console.error('   ‚ùå Error inserting requirements:', insertError);
            throw insertError;
        }

        console.log(`   ‚úÖ Saved ${inserted.length} requirements to database\n`);

        // Summary
        console.log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
        console.log('‚úÖ PARSING COMPLETED SUCCESSFULLY!');
        console.log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
        console.log(`üìÑ File: ${fileUsed.fileName}`);
        console.log(`üìä Text extracted: ${text.length} characters`);
        console.log(`üéØ Requirements extracted: ${requirements.length}`);
        console.log(`üíæ Requirements saved: ${inserted.length}`);
        console.log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n');

        // Show sample requirements
        console.log('üìã Sample requirements:\n');
        inserted.slice(0, 3).forEach((req, idx) => {
            console.log(`${idx + 1}. [${req.clause}] ${req.requirementTextShort}`);
            console.log(`   System: ${req.systemId} | Method: ${req.checkMethod}\n`);
        });

    } catch (error) {
        console.error('\n‚ùå ERROR:', error.message);
        console.error('\nStack:', error.stack);
        process.exit(1);
    }
}

// Run the parser
parsePdfWithGpt();
